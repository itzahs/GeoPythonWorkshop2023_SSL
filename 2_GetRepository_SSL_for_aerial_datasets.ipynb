{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itzahs/GeoPythonWorkshop2023_SSL/blob/main/2_GetRepository_SSL_for_aerial_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GeoPython 2023 Workshop\n",
        "\n",
        "## Section 1: Download the UCM dataset & understand augmentations\n",
        "## Section 2: Implementation of the FixMatch algorithm using PyTorch "
      ],
      "metadata": {
        "id": "6ZT8hosUQMrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages and libraries \n"
      ],
      "metadata": {
        "id": "G0El0qJbZ7g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import \n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "3IKIeIObZ6AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor GPU usage"
      ],
      "metadata": {
        "id": "xaNsN14-RV9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Monitor the GPU usage \n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JR-1kDgYMX2",
        "outputId": "8b4b8589-1c4f-47e3-f021-d0778de39496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar  5 20:13:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    23W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up working folder"
      ],
      "metadata": {
        "id": "nsHh9JbXZzYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create working folder \n",
        "folder_name = \"GeoPythonWorkshop_SSL\"\n",
        "path_to_folder = \"/content/drive/MyDrive/\" + folder_name\n",
        "\n",
        "if not os.path.exists(path_to_folder):\n",
        "    os.makedirs(path_to_folder)\n",
        "    print(F\"Folder '{folder_name}' has been created successfully.\")\n",
        "else:\n",
        "    print(F\"Folder '{folder_name}' already exists.\")\n",
        "\n",
        "# Set working folder as default\n",
        "%cd \"/content/drive/MyDrive/GeoPythonWorkshop_SSL\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm9aZ1AwZyEm",
        "outputId": "ddb8901d-5bb8-48df-e528-c75e0eeb368f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Folder 'GeoPythonWorkshop_SSL' already exists.\n",
            "/content/drive/MyDrive/GeoPythonWorkshop_SSL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone repository"
      ],
      "metadata": {
        "id": "D64vQ4qoaQ-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TencentYoutuResearch/Classification-SemiCLS.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqcNSb_paJ5l",
        "outputId": "900505a5-bb5a-4dc5-832f-cbe0fe9e4826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Classification-SemiCLS' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cloned repository as default\n",
        "%cd \"/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS\"\n",
        "\n",
        "# Checking that repository was correctly cloned\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROM5xdWHs4vn",
        "outputId": "978fb0f6-4315-47a5-975f-a685f3d3d0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS\n",
            "configs  loss\t optimizer\t    requirements.txt  tools\t     utils\n",
            "dataset  mmcv\t pretrained_models  results\t      trainer\n",
            "LICENSE  models  README.md\t    scheduler\t      train_semi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install all the requirements"
      ],
      "metadata": {
        "id": "M82_MM8vBzrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/open-mmlab/mmcv.git\n",
        "%cd mmcv/\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1OtZAZ8eXwe",
        "outputId": "7f0a5ee0-cb47-4b2f-8310-522e32a1832e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mmcv' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/mmcv\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/mmcv\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.8/dist-packages (from mmcv==1.7.1) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mmcv==1.7.1) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mmcv==1.7.1) (23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mmcv==1.7.1) (8.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mmcv==1.7.1) (6.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.8/dist-packages (from mmcv==1.7.1) (0.32.0)\n",
            "Installing collected packages: mmcv\n",
            "  Attempting uninstall: mmcv\n",
            "    Found existing installation: mmcv 1.7.1\n",
            "    Can't uninstall 'mmcv'. No files were found to uninstall.\n",
            "  Running setup.py develop for mmcv\n",
            "Successfully installed mmcv-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install other libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install apex\n",
        "!pip install tensorboardX\n",
        "!pip install tensorboard\n",
        "!pip install tensorrt\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-5pc2ZRf6L5",
        "outputId": "b42ca06e-65ae-4ad5-9557-f4e4012b3f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apex in /usr/local/lib/python3.8/dist-packages (0.9.10.dev0)\n",
            "Requirement already satisfied: pyramid>1.1.2 in /usr/local/lib/python3.8/dist-packages (from apex) (2.0.1)\n",
            "Requirement already satisfied: pyramid-mailer in /usr/local/lib/python3.8/dist-packages (from apex) (0.15.1)\n",
            "Requirement already satisfied: zope.sqlalchemy in /usr/local/lib/python3.8/dist-packages (from apex) (2.0)\n",
            "Requirement already satisfied: wtforms-recaptcha in /usr/local/lib/python3.8/dist-packages (from apex) (0.3.2)\n",
            "Requirement already satisfied: cryptacular in /usr/local/lib/python3.8/dist-packages (from apex) (1.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from apex) (2.25.1)\n",
            "Requirement already satisfied: velruse>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from apex) (1.1.1)\n",
            "Requirement already satisfied: wtforms in /usr/local/lib/python3.8/dist-packages (from apex) (3.0.1)\n",
            "Requirement already satisfied: zope.interface>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (5.5.2)\n",
            "Requirement already satisfied: webob>=1.8.3 in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (1.8.7)\n",
            "Requirement already satisfied: translationstring>=0.4 in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (1.4)\n",
            "Requirement already satisfied: plaster-pastedeploy in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (1.0.1)\n",
            "Requirement already satisfied: venusian>=1.0 in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (3.0.0)\n",
            "Requirement already satisfied: plaster in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (57.4.0)\n",
            "Requirement already satisfied: zope.deprecation>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (4.4.0)\n",
            "Requirement already satisfied: hupper>=1.5 in /usr/local/lib/python3.8/dist-packages (from pyramid>1.1.2->apex) (1.11)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from velruse>=1.0.3->apex) (1.3.1)\n",
            "Requirement already satisfied: anykeystore in /usr/local/lib/python3.8/dist-packages (from velruse>=1.0.3->apex) (0.2)\n",
            "Requirement already satisfied: python3-openid in /usr/local/lib/python3.8/dist-packages (from velruse>=1.0.3->apex) (3.2.0)\n",
            "Requirement already satisfied: pbkdf2 in /usr/local/lib/python3.8/dist-packages (from cryptacular->apex) (1.3)\n",
            "Requirement already satisfied: repoze.sendmail>=4.1 in /usr/local/lib/python3.8/dist-packages (from pyramid-mailer->apex) (4.4.1)\n",
            "Requirement already satisfied: transaction in /usr/local/lib/python3.8/dist-packages (from pyramid-mailer->apex) (3.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->apex) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->apex) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->apex) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->apex) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.8/dist-packages (from wtforms->apex) (2.1.2)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,<2,>=1.1 in /usr/local/lib/python3.8/dist-packages (from zope.sqlalchemy->apex) (1.4.46)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,<2,>=1.1->zope.sqlalchemy->apex) (2.0.2)\n",
            "Requirement already satisfied: PasteDeploy>=2.0 in /usr/local/lib/python3.8/dist-packages (from plaster-pastedeploy->pyramid>1.1.2->apex) (3.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (2.6)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.11.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.22.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.16.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.8/dist-packages (8.5.3.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu11 in /usr/local/lib/python3.8/dist-packages (from tensorrt) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11 in /usr/local/lib/python3.8/dist-packages (from tensorrt) (8.8.0.121)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11 in /usr/local/lib/python3.8/dist-packages (from tensorrt) (11.8.89)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code modifications\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BlotIX9LfFWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Config \n",
        "The config code defines a configuration for training a deep learning model using the FixMatch algorithm on the UCM dataset. The model architecture used is a wideresnet28x2. The training will be performed on a single GPU with a batch size of 8. Only 4 labeled samples per class will be used for training.\n",
        "The training process is organized into three main components:  \n",
        "\n",
        "*   Train: Algorithm, number of steps and loss function.\n",
        "*   Model: Architecture of the model to be trained. \n",
        "*   Data:  Loading and preprocessing the UCM dataset, number of labeled samples, batch size and data augmentation pipeline. \n",
        "\n",
        "Other options such as the learning rate scheduler, exponential moving average (EMA) of model parameters, automatic mixed precision (AMP) optimization, and logging and checkpointing options are also defined.\n",
        "\n",
        "Finally, an optimizer of type SGD is defined with a learning rate of 0.03, momentum of 0.9, weight decay of 0.001, and Nesterov momentum enabled."
      ],
      "metadata": {
        "id": "wWYDWDffHeFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset builder and training syntax \n",
        "\n",
        "For the code to work in a Jupyter notebook environment we need to make two modifications. First, Jupyter notebook creates .ipynb_checkpoints and we need to ignore them and then, make some changes in the code syntax.\n",
        "1. In dataset/builder.py (lines 15): Add import os\n",
        "2. In dataset/builder.py (lines 82-92): add check to ignore .ipynb_checkpoints.\n",
        "3. In train_semi.py (lines 444 & 450): Modify labeled.next() for next(labeled).\n",
        "4. In train_semi.py (lines 453 & 459): Modify unlabeled.next for next(unlabeled).\n"
      ],
      "metadata": {
        "id": "SkFV70X0q9XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write automatically the changes in:\n",
        "1. ./Classification-SemiCLS/configs/fm_ucm.py\n",
        "2. ./Classification-SemiCLS/dataset/builder.py\n",
        "3. ./Classification-SemiCLS/train_semi.py. \n"
      ],
      "metadata": {
        "id": "jsDFsISHtep1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set working folder as default\n",
        "%cd \"/content/drive/MyDrive/GeoPythonWorkshop_SSL\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bv22StHIP_",
        "outputId": "3e57effc-e39f-426e-ad07-7d124ff0fa74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GeoPythonWorkshop_SSL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./Classification-SemiCLS/configs/fm_ucm.py\n",
        "\n",
        "\"\"\" The Code is under Tencent Youtu Public Rule\"\"\"\n",
        "\n",
        "train = dict(eval_step=1024,\n",
        "             total_steps=1024*20,\n",
        "             trainer=dict(type=\"FixMatch\",\n",
        "                          threshold=0.95,\n",
        "                          T=1.,\n",
        "                          lambda_u=1.,\n",
        "                          loss_x=dict(\n",
        "                              type=\"cross_entropy\",\n",
        "                              reduction=\"mean\"),\n",
        "                          loss_u=dict(\n",
        "                              type=\"cross_entropy\",\n",
        "                              reduction=\"none\"),\n",
        "                          ))\n",
        "num_classes = 21\n",
        "\n",
        "model = dict(\n",
        "     type=\"wideresnet\",\n",
        "     depth=28,\n",
        "     widen_factor=2,\n",
        "     dropout=0,\n",
        "     num_classes=num_classes,\n",
        ")\n",
        "\n",
        "ucm_mean = (0.485, 0.456, 0.406) \n",
        "ucm_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "data = dict(\n",
        "    type=\"MyDataset\",\n",
        "    num_workers=0,\n",
        "    num_labeled=84,\n",
        "    num_classes=num_classes,\n",
        "    batch_size=8,\n",
        "    expand_labels=False,\n",
        "    mu=7,\n",
        "\n",
        "    root=\"./UCMerced_LandUse/Images\",\n",
        "    labeled_names_file=\"./UCMerced_LandUse/Images/UCM_train.txt\",\n",
        "    test_names_file=\"./UCMerced_LandUse/Images/UCM_test.txt\",\n",
        "    lpipelines=[[\n",
        "        dict(type=\"RandomHorizontalFlip\"),\n",
        "        dict(type=\"RandomResizedCrop\", size=224, scale=(0.2, 1.0)),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "    ]],\n",
        "    upipelinse=[[\n",
        "        dict(type=\"RandomHorizontalFlip\"),\n",
        "        dict(type=\"Resize\", size=256),\n",
        "        dict(type=\"CenterCrop\", size=224),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "        ],\n",
        "        [\n",
        "        dict(type=\"RandomHorizontalFlip\"),\n",
        "        dict(type=\"RandomResizedCrop\", size=224, scale=(0.2, 1.0)),\n",
        "        dict(type=\"RandAugmentMC\", n=2, m=10),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "    ]],\n",
        "    vpipeline=[\n",
        "        dict(type=\"Resize\", size=256),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "    ])\n",
        "\n",
        "scheduler = dict(\n",
        "    type='cosine_schedule_with_warmup',\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=train['total_steps']\n",
        ")\n",
        "\n",
        "ema = dict(use=True, pseudo_with_ema=False, decay=0.999)\n",
        "#apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "#\"See details at https://nvidia.github.io/apex/amp.html\n",
        "amp = dict(use=False, opt_level=\"O1\")\n",
        "\n",
        "log = dict(interval=1)\n",
        "ckpt = dict(interval=1000)\n",
        "\n",
        "# optimizer\n",
        "optimizer = dict(type='SGD', lr=0.03, momentum=0.9, weight_decay=0.001, nesterov=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLR-cMAYFIPC",
        "outputId": "e18e929d-9968-4be6-99bd-08810418b538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./Classification-SemiCLS/configs/fm_ucm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./Classification-SemiCLS/dataset/builder.py\n",
        "\n",
        "\"\"\" The Code is under Tencent Youtu Public Rule\n",
        "This file build a dataset for semi supervised learning\n",
        "dataset_cfg:\n",
        "    num_labeled:\n",
        "    num_classes:\n",
        "    batch_size:\n",
        "    eval_step:\n",
        "\n",
        "    root:\n",
        "    type: cifar10\n",
        "    lpipelines:\n",
        "    upipelinse:\n",
        "    vpipeline:\n",
        "\"\"\"\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "from dataset.cifar import CIFAR10SSL, CIFAR100SSL, x_u_split\n",
        "from dataset.imagenet import get_imagenet_ssl_dataset\n",
        "from dataset.MyDataset import MyDataset\n",
        "from dataset.stl10 import get_stl10\n",
        "from dataset.transforms.builder import BaseTransform, ListTransform\n",
        "from dataset.txt_dataset import get_txt_ssl_dataset\n",
        "\n",
        "dataset_dict = {\"CIFAR10SSL\": CIFAR10SSL, \"CIFAR100SSL\": CIFAR100SSL}\n",
        "\n",
        "base_dict = {\n",
        "    \"CIFAR10\": datasets.CIFAR10,\n",
        "    \"CIFAR10SSL\": datasets.CIFAR10,\n",
        "    \"CIFAR100\": datasets.CIFAR100,\n",
        "    \"CIFAR100SSL\": datasets.CIFAR100,\n",
        "    \"MyDataset\": MyDataset,\n",
        "}\n",
        "\n",
        "def build(cfg):\n",
        "\n",
        "    if cfg.type == \"MyDataset\":\n",
        "        transform_ulabeled = ListTransform(cfg.upipelinse)\n",
        "        train_unlabeled_dataset = datasets.ImageFolder(\n",
        "            root=cfg.root,\n",
        "            transform=transform_ulabeled)\n",
        "        transform_labeled = ListTransform(cfg.lpipelines)\n",
        "        train_labeled_dataset = MyDataset(names_file=cfg.labeled_names_file, transform=transform_labeled)\n",
        "        transform_val = BaseTransform(cfg.vpipeline)\n",
        "        test_dataset = MyDataset(names_file=cfg.test_names_file, transform=transform_val)\n",
        "\n",
        "    elif cfg.type == \"ImagenetSSL\":\n",
        "        transform_labeled = ListTransform(cfg.lpipelines)\n",
        "        transform_ulabeled = ListTransform(cfg.upipelinse)\n",
        "        transform_val = BaseTransform(cfg.vpipeline)\n",
        "        return get_imagenet_ssl_dataset(root=cfg.root,\n",
        "                                        percent=cfg.percent,\n",
        "                                        anno_file=cfg.anno_file,\n",
        "                                        transform_labeled=transform_labeled,\n",
        "                                        transform_ulabeled=transform_ulabeled,\n",
        "                                        transform_val=transform_val)\n",
        "\n",
        "    elif cfg.type == \"TxtDatasetSSL\":\n",
        "        transform_labeled = ListTransform(cfg.lpipelines)\n",
        "        transform_ulabeled = ListTransform(cfg.upipelinse)\n",
        "        transform_val = BaseTransform(cfg.vpipeline)\n",
        "        return get_txt_ssl_dataset(\n",
        "            l_anno_file=cfg.l_anno_file,\n",
        "            u_anno_file=cfg.u_anno_file,\n",
        "            v_anno_file=cfg.v_anno_file,\n",
        "            transform_labeled=transform_labeled,\n",
        "            transform_ulabeled=transform_ulabeled,\n",
        "            transform_val=transform_val)\n",
        "\n",
        "    elif cfg.type == \"STL10SSL\":\n",
        "        transform_labeled = ListTransform(cfg.lpipelines)\n",
        "        transform_ulabeled = ListTransform(cfg.upipelinse)\n",
        "        transform_val = BaseTransform(cfg.vpipeline)\n",
        "        return get_stl10(\n",
        "            root=cfg.root, folds=cfg.folds,\n",
        "            transform_labeled=transform_labeled,\n",
        "            transform_ulabeled=transform_ulabeled,\n",
        "            transform_val=transform_val)\n",
        "\n",
        "    else:\n",
        "\n",
        "        # check if .ipynb_checkpoints is in root, and exclude it\n",
        "        root = os.path.join(cfg.root, '')\n",
        "        if os.path.isdir(os.path.join(root, \".ipynb_checkpoints\")):\n",
        "            exclude_dir = os.path.join(root, \".ipynb_checkpoints\")\n",
        "        else:\n",
        "            exclude_dir = None\n",
        "\n",
        "        base_dataset = base_dict[cfg.type](cfg.root, train=True, download=True)\n",
        "\n",
        "        train_labeled_idxs, train_unlabeled_idxs = x_u_split(\n",
        "            cfg, base_dataset.targets)\n",
        "\n",
        "        # init labeled datasets\n",
        "        transform_labeled = ListTransform(cfg.lpipelines)\n",
        "        train_labeled_dataset = dataset_dict[cfg.type](\n",
        "            cfg.root,\n",
        "            train_labeled_idxs,\n",
        "            train=True,\n",
        "            transform=transform_labeled,\n",
        "            anno_file=cfg.lanno_file if cfg.get(\"lanno_file\", False) else None)\n",
        "\n",
        "        transform_ulabeled = ListTransform(cfg.upipelinse)\n",
        "        train_unlabeled_dataset = dataset_dict[cfg.type](\n",
        "            cfg.root,\n",
        "            train_unlabeled_idxs,\n",
        "            train=True,\n",
        "            transform=transform_ulabeled,\n",
        "            anno_file=cfg.uanno_file if cfg.get(\"uanno_file\", False) else None)\n",
        "\n",
        "        transform_val = BaseTransform(cfg.vpipeline)\n",
        "        test_dataset = base_dict[cfg.type](cfg.root,\n",
        "                                        train=False,\n",
        "                                        transform=transform_val,\n",
        "                                        download=False)\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGlF1Z_kIvwF",
        "outputId": "9089c0fc-3d1b-4217-fd78-16d3edf46c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./Classification-SemiCLS/dataset/builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./Classification-SemiCLS/train_semi.py  \n",
        "\n",
        "\"\"\" The Code is under Tencent Youtu Public Rule\n",
        "\n",
        "this is the mian sript in our SSL tool box\n",
        "The interface contains many args\n",
        "\n",
        "$CONFIG_PATH=/your/config/path\n",
        "$OUTPUT_PATH=/your/output/path\n",
        "For single gpu\n",
        "number_of_gpus=1\n",
        "python3 train_semi.py --cfg $CONFIG_PATH  --out $OUTPUT_PATH --seed 5\n",
        "\n",
        "For multi gpu\n",
        "number_of_gpus=N\n",
        "python3 -m torch.distributed.launch --nproc_per_node $number_of_gpus \\\n",
        "    train_semi.py\\\n",
        "        --cfg $CONFIG_PATH\n",
        "        --out $OUTPUT_PATH --use_BN True  --seed 5\n",
        "\n",
        "\"\"\"\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from calendar import c\n",
        "from sched import scheduler\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from mmcv import Config\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import builder as dataset_builder\n",
        "from models import builder as model_builder\n",
        "from optimizer import builder as optim_builder\n",
        "from scheduler import builder as scheduler_builder\n",
        "from trainer import builder as trainer_builder\n",
        "from utils import AverageMeter, AverageMeterManeger, accuracy\n",
        "from utils.ckpt_utils import save_ckpt_dict\n",
        "from utils.config_utils import overwrite_config\n",
        "from utils.log_utils import get_default_logger\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# global variables\n",
        "global logger\n",
        "best_acc = 0\n",
        "SCALER = None\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    \"\"\" set seed for the whole program for removing randomness\n",
        "    \"\"\"\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def get_test_model(ema_model, model, use_ema):\n",
        "    \"\"\" use ema model or test model\n",
        "    \"\"\"\n",
        "    if use_ema:\n",
        "        test_model = ema_model.ema\n",
        "        test_prefix = \"ema\"\n",
        "    else:\n",
        "        test_model = model\n",
        "        test_prefix = \"\"\n",
        "    return test_model, test_prefix\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = get_args()\n",
        "    global best_acc\n",
        "\n",
        "    # prepare config and make output dir\n",
        "    cfg = Config.fromfile(args.cfg)\n",
        "    cfg = overwrite_config(cfg, args.other_args)\n",
        "    cfg.resume = args.resume\n",
        "    cfg.data['eval_step'] = cfg.train.eval_step\n",
        "\n",
        "    # set amp scaler, usually no use\n",
        "    global SCALER\n",
        "    if args.fp16:\n",
        "        SCALER = torch.cuda.amp.GradScaler()\n",
        "    else:\n",
        "        SCALER = None\n",
        "\n",
        "    # set summary writer on rank 0\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.out, exist_ok=True)\n",
        "        args.writer = SummaryWriter(args.out)\n",
        "        cfg.dump(os.path.join(args.out, os.path.basename(args.cfg)))\n",
        "\n",
        "    # set up logger\n",
        "    global logger\n",
        "    logger = get_default_logger(\n",
        "        args=args,\n",
        "        logger_name='root',\n",
        "        default_level=logging.INFO\n",
        "        if args.local_rank in [-1, 0] else logging.WARN,\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        rank=args.local_rank)\n",
        "\n",
        "    #set random seed\n",
        "    if args.seed is not None:\n",
        "        cfg.seed = args.seed\n",
        "    elif cfg.get(\"seed\", None) is not None:\n",
        "        args.seed = cfg.seed\n",
        "\n",
        "    # set folds for stl10 dataset if used\n",
        "    if \"folds\" in cfg.data.keys():\n",
        "        cfg.data.folds = cfg.seed\n",
        "\n",
        "    args.amp = False\n",
        "    if cfg.get(\"amp\", False) and cfg.amp.use:\n",
        "        args.amp = True\n",
        "        args.opt_level = cfg.amp.opt_level\n",
        "\n",
        "    if not args.pretrained and \"pretrained\" in cfg.keys():\n",
        "        args.pretrained = cfg.pretrained\n",
        "\n",
        "    args.total_steps = cfg.train.total_steps\n",
        "    args.eval_steps = cfg.train.eval_step\n",
        "\n",
        "    # init dist params\n",
        "    if args.local_rank == -1:\n",
        "        device = torch.device('cuda', args.gpu_id)\n",
        "        args.world_size = 1\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device('cuda', args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.world_size = torch.distributed.get_world_size()\n",
        "        args.n_gpu = 1\n",
        "\n",
        "    # set device\n",
        "    args.device = device\n",
        "\n",
        "    logger.warning(\n",
        "        f\"Process rank: {args.local_rank}, \"\n",
        "        f\"device: {args.device}, \"\n",
        "        f\"n_gpu: {args.n_gpu}, \"\n",
        "        f\"distributed training: {bool(args.local_rank != -1)}, \"\n",
        "        f\"16-bits training: {args.amp}\", )\n",
        "\n",
        "    logger.info(dict(args._get_kwargs()))\n",
        "\n",
        "    if args.seed is not None:\n",
        "        set_seed(args)\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    # make dataset\n",
        "    #labeled_dataset, unlabeled_dataset, test_dataset = dataset_builder.build(cfg.data)\n",
        "    labeled_dataset, unlabeled_dataset, test_dataset = dataset_builder.build(\n",
        "        cfg.data)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    # make dataset loader\n",
        "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
        "\n",
        "    labeled_trainloader, unlabeled_trainloader, test_loader = get_dataloader(\n",
        "        cfg, train_sampler, labeled_dataset, unlabeled_dataset, test_dataset)\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model = model_builder.build(cfg.model)\n",
        "\n",
        "    # load from pre-trained, before DistributedDataParallel constructor\n",
        "    if args.pretrained:\n",
        "        if os.path.isfile(args.pretrained):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
        "            checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n",
        "\n",
        "            # rename moco pre-trained keys\n",
        "            state_dict = checkpoint['state_dict']\n",
        "            for k in list(state_dict.keys()):\n",
        "                # retain only encoder_q up to before the embedding layer\n",
        "                if k.startswith('module.encoder_q'\n",
        "                                ) and not k.startswith('module.encoder_q.fc'):\n",
        "                    # remove prefix\n",
        "                    state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n",
        "                # delete renamed or unused k\n",
        "                del state_dict[k]\n",
        "\n",
        "            msg = model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Missing keys\", msg.missing_keys)\n",
        "\n",
        "            print(\"=> loaded pre-trained model '{}'\".format(args.pretrained))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    # make optimizer,scheduler\n",
        "    optimizer = optim_builder.build(cfg.optimizer, model)\n",
        "    scheduler = set_scheduler(cfg, args, optimizer)\n",
        "\n",
        "    # set ema\n",
        "    args.use_ema = False\n",
        "    ema_model = None\n",
        "    if cfg.get(\"ema\", False) and cfg.ema.use:\n",
        "        args.use_ema = True\n",
        "        from models.ema import ModelEMA\n",
        "        ema_model = ModelEMA(args.device, model, cfg.ema.decay)\n",
        "\n",
        "    args.start_epoch = 0\n",
        "\n",
        "    # initialize from resume for fixed info and task_specific_info\n",
        "    task_specific_info = dict()\n",
        "\n",
        "    if args.resume:\n",
        "        if args.use_ema:\n",
        "            resume(args,\n",
        "                   model,\n",
        "                   optimizer,\n",
        "                   scheduler,\n",
        "                   task_specific_info,\n",
        "                   ema_model=ema_model)\n",
        "        else:\n",
        "            resume(args, model, optimizer, scheduler, task_specific_info)\n",
        "\n",
        "    # builde model trainer\n",
        "    cfg.train.trainer['amp'] = args.amp\n",
        "    model_trainer = trainer_builder.build(cfg.train.trainer)(device=device,\n",
        "                                                             all_cfg=cfg)\n",
        "\n",
        "    # process model\n",
        "    if args.amp:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(model,\n",
        "                                          optimizer,\n",
        "                                          opt_level=args.opt_level)\n",
        "    #use_BN\n",
        "    use_batchnorm = args.use_BN\n",
        "    if use_batchnorm:\n",
        "        model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model,\n",
        "            device_ids=[args.local_rank],\n",
        "            output_device=args.local_rank,\n",
        "            find_unused_parameters=True)\n",
        "\n",
        "    log_info(cfg, args)\n",
        "\n",
        "    model.zero_grad()\n",
        "    #train loop\n",
        "    train(args, cfg, labeled_trainloader, unlabeled_trainloader, test_loader,\n",
        "          model, optimizer, ema_model, scheduler, model_trainer,\n",
        "          task_specific_info)\n",
        "\n",
        "\n",
        "# resume from checkpoint\n",
        "def resume(args,\n",
        "           model,\n",
        "           optimizer,\n",
        "           scheduler,\n",
        "           task_specific_info,\n",
        "           ema_model=None):\n",
        "    logger.info(\"==> Resuming from checkpoint..\")\n",
        "    if not os.path.isfile(args.resume):\n",
        "        logger.info(\"Error resuming from {}! try resume from last one\".format(\n",
        "            args.resume))\n",
        "        args.resume = os.path.join(args.out, \"checkpoint.pth.tar\")\n",
        "        logger.info(\"Resuming from {}\".format(args.resume))\n",
        "\n",
        "    # try resume if specified\n",
        "    if not os.path.isfile(args.resume):\n",
        "        logger.info(\"Failed to resume from {}\".format(args.resume))\n",
        "    else:\n",
        "        args.out = os.path.dirname(args.resume)\n",
        "        checkpoint = torch.load(args.resume)\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        args.start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        if args.use_ema:\n",
        "            ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "        for key in checkpoint.keys():\n",
        "            if key not in [\n",
        "                    'epoch', 'state_dict', 'ema_state_dict', 'acc', 'best_acc',\n",
        "                    'optimizer', 'scheduler'\n",
        "            ]:\n",
        "                task_specific_info[key] = checkpoint[key]\n",
        "                try:\n",
        "                    task_specific_info[key] = task_specific_info[key].to(\n",
        "                        args.device)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "\n",
        "#set scheduler\n",
        "def set_scheduler(cfg, args, optimizer):\n",
        "    args.epochs = math.ceil(cfg.train.total_steps / cfg.train.eval_step)\n",
        "    args.eval_step = cfg.train.eval_step\n",
        "    args.total_steps = cfg.train.total_steps\n",
        "    scheduler = scheduler_builder.build(cfg.scheduler)(optimizer=optimizer)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "# log info before training\n",
        "def log_info(cfg, args):\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Task = {cfg.data.type}\")\n",
        "    if \"num_labeled\" in cfg.data.keys():\n",
        "        logging_num_labeled = cfg.data.num_labeled\n",
        "    elif \"percent\" in cfg.data.keys():\n",
        "        logging_num_labeled = \"{}%\".format(cfg.data.percent)\n",
        "    else:\n",
        "        logging_num_labeled = \"seed {}\".format(cfg.seed)\n",
        "\n",
        "    logger.info(f\"  Num Label = {logging_num_labeled}\")\n",
        "    logger.info(f\"  Num Epochs = {args.epochs}\")\n",
        "    logger.info(f\"  Batch size per GPU = {cfg.data.batch_size}\")\n",
        "    logger.info(\n",
        "        f\"  Total train batch size = {cfg.data.batch_size*args.world_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.total_steps}\")\n",
        "    logger.info(cfg)\n",
        "\n",
        "\n",
        "# get args\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch FixMatch Training')\n",
        "    parser.add_argument('--cfg', type=str, required=True, help='a config')\n",
        "    parser.add_argument('--gpu-id',\n",
        "                        default='0',\n",
        "                        type=int,\n",
        "                        help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "    parser.add_argument('--out',\n",
        "                        default='result',\n",
        "                        help='directory to output the result')\n",
        "    parser.add_argument('--pretrained',\n",
        "                        default=None,\n",
        "                        help='directory to pretrained model')\n",
        "    parser.add_argument('--resume',\n",
        "                        default='',\n",
        "                        type=str,\n",
        "                        help='path to latest checkpoint (default: none)')\n",
        "    parser.add_argument('--seed', default=None, type=int, help=\"random seed\")\n",
        "    parser.add_argument('--use_BN',\n",
        "                        default=False,\n",
        "                        type=bool,\n",
        "                        help=\"use_batchnorm\")\n",
        "    parser.add_argument('--fp16',\n",
        "                        action='store_true',\n",
        "                        help=\"whether use fp16 for training\")\n",
        "    parser.add_argument(\"--local_rank\",\n",
        "                        type=int,\n",
        "                        default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--no-progress',\n",
        "                        action='store_true',\n",
        "                        help=\"don't use progress bar\")\n",
        "    parser.add_argument(\n",
        "        '--other-args',\n",
        "        default='',\n",
        "        type=str,\n",
        "        help='other args to overwrite the config, keys are split \\\n",
        "                            by space and args split by |, such as \\'seed 1|train trainer T 1|\\' '\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "# labeled_trainloader,unlabeled_trainloader,test_loader\n",
        "def get_dataloader(cfg, train_sampler, labeled_dataset, unlabeled_dataset,\n",
        "                   test_dataset):\n",
        "    # prepare labeled_trainloader\n",
        "    labeled_trainloader = DataLoader(labeled_dataset,\n",
        "                                     sampler=train_sampler(labeled_dataset),\n",
        "                                     batch_size=cfg.data.batch_size,\n",
        "                                     num_workers=cfg.data.num_workers,\n",
        "                                     drop_last=True)\n",
        "    # prepare unlabeled_trainloader\n",
        "    unlabeled_trainloader = DataLoader(\n",
        "        unlabeled_dataset,\n",
        "        sampler=train_sampler(unlabeled_dataset),\n",
        "        batch_size=cfg.data.batch_size * cfg.data.mu,\n",
        "        num_workers=cfg.data.num_workers,\n",
        "        drop_last=True)\n",
        "    # prepare test_loader\n",
        "    test_loader = DataLoader(test_dataset,\n",
        "                             sampler=SequentialSampler(test_dataset),\n",
        "                             batch_size=cfg.data.batch_size,\n",
        "                             num_workers=cfg.data.num_workers)\n",
        "\n",
        "    return labeled_trainloader, unlabeled_trainloader, test_loader\n",
        "\n",
        "\n",
        "# train_loop\n",
        "def train(args, cfg, labeled_trainloader, unlabeled_trainloader, test_loader,\n",
        "          model, optimizer, ema_model, scheduler, model_trainer,\n",
        "          task_specific_info):\n",
        "    \"\"\"\n",
        "    Train function for training\n",
        "    \"\"\"\n",
        "    global best_acc\n",
        "    test_accs = []\n",
        "\n",
        "    if args.world_size > 1:\n",
        "        labeled_epoch = 0\n",
        "        unlabeled_epoch = 0\n",
        "        labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
        "        unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
        "\n",
        "    labeled_iter = iter(labeled_trainloader)\n",
        "    unlabeled_iter = iter(unlabeled_trainloader)\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        # init logger\n",
        "        meter_manager = AverageMeterManeger()\n",
        "        meter_manager.register('batch_time')\n",
        "        meter_manager.register('data_time')\n",
        "        end = time.time()\n",
        "        model.train()\n",
        "        if not args.no_progress:\n",
        "            # p_bar = tqdm(range(args.eval_step),\n",
        "            #              disable=args.local_rank not in [-1, 0])\n",
        "            pass\n",
        "        for batch_idx in range(args.eval_step):\n",
        "            try:\n",
        "                data_x = next(labeled_iter)\n",
        "            except Exception:\n",
        "                if args.world_size > 1:\n",
        "                    labeled_epoch += 1\n",
        "                    labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
        "                labeled_iter = iter(labeled_trainloader)\n",
        "                data_x = next(labeled_iter)\n",
        "\n",
        "            try:\n",
        "                data_u = next(unlabeled_iter)\n",
        "            except Exception:\n",
        "                if args.world_size > 1:\n",
        "                    unlabeled_epoch += 1\n",
        "                    unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
        "                unlabeled_iter = iter(unlabeled_trainloader)\n",
        "                data_u = next(unlabeled_iter)\n",
        "\n",
        "            meter_manager.data_time.update(time.time() - end)\n",
        "            # calculate loss\n",
        "            loss_dict = model_trainer.compute_loss(\n",
        "                data_x=data_x,\n",
        "                data_u=data_u,\n",
        "                model=model,\n",
        "                optimizer=optimizer,\n",
        "                epoch=epoch,\n",
        "                iter=batch_idx,\n",
        "                ema_model=ema_model,\n",
        "                task_specific_info=task_specific_info,\n",
        "                SCALER=SCALER if SCALER is not None else None)\n",
        "\n",
        "            # update logger\n",
        "            meter_manager.try_register_and_update(loss_dict)\n",
        "\n",
        "            # step\n",
        "            if SCALER is not None:\n",
        "                SCALER.step(optimizer)\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Updates the scale for next iteration\n",
        "            if SCALER is not None:\n",
        "                SCALER.update()\n",
        "\n",
        "            # update ema if needed\n",
        "            if args.use_ema:\n",
        "                ema_model.update(model)\n",
        "            model.zero_grad()\n",
        "\n",
        "            meter_manager.batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if not args.no_progress and args.local_rank in [-1, 0]:\n",
        "                if batch_idx % cfg.log.interval == 0:\n",
        "                    meter_desc = meter_manager.get_desc()\n",
        "                    logger.info(\n",
        "                        \"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.4f} {desc}\"\n",
        "                        .format(epoch=epoch + 1,\n",
        "                                epochs=args.epochs,\n",
        "                                batch=batch_idx + 1,\n",
        "                                iter=args.eval_step,\n",
        "                                lr=scheduler.get_last_lr()[0],\n",
        "                                desc=meter_desc))\n",
        "                    # p_bar.update()\n",
        "\n",
        "        # if not args.no_progress:\n",
        "        #     p_bar.close()\n",
        "\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            meter_manager.add_to_writer(args.writer, epoch, prefix=\"train/\")\n",
        "\n",
        "            # add test info\n",
        "            test_model, test_prefix = get_test_model(ema_model, model,\n",
        "                                                     args.use_ema)\n",
        "            test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
        "            args.writer.add_scalar('test/1.{}_acc'.format(test_prefix),\n",
        "                                   test_acc, epoch)\n",
        "            args.writer.add_scalar('test/2.{}_loss'.format(test_prefix),\n",
        "                                   test_loss, epoch)\n",
        "            is_best = test_acc > best_acc\n",
        "            best_acc = max(test_acc, best_acc)\n",
        "\n",
        "            # add second evaluatino writer needed\n",
        "            if cfg.get(\"evaluation\", False) and cfg.evaluation.eval_both:\n",
        "                test_model_second, test_prefix_second = get_test_model(\n",
        "                    ema_model, model, not args.use_ema)\n",
        "                test_loss_second, test_acc_second = test(\n",
        "                    args, test_loader, test_model_second, epoch)\n",
        "                args.writer.add_scalar(\n",
        "                    'test/1.{}_acc'.format(test_prefix_second),\n",
        "                    test_acc_second, epoch)\n",
        "                args.writer.add_scalar(\n",
        "                    'test/2.{}_loss'.format(test_prefix_second),\n",
        "                    test_loss_second, epoch)\n",
        "\n",
        "            # save model\n",
        "            if epoch % cfg.ckpt.interval == 0:\n",
        "                save_ckpt_dict(args, model, ema_model, epoch, test_acc,\n",
        "                               optimizer, scheduler, task_specific_info,\n",
        "                               is_best, best_acc)\n",
        "\n",
        "            test_accs.append(test_acc)\n",
        "            logger.info('Best top-1 acc: {:.2f}'.format(best_acc))\n",
        "            logger.info('Mean top-1 acc: {:.2f}\\n'.format(\n",
        "                np.mean(test_accs[-20:])))\n",
        "\n",
        "    # save last ckpt\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        save_ckpt_dict(args, model, ema_model, epoch, test_acc, optimizer,\n",
        "                       scheduler, task_specific_info, is_best, best_acc)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        args.writer.close()\n",
        "\n",
        "\n",
        "#test/valiadate step\n",
        "\n",
        "\n",
        "def test(args, test_loader, model, epoch):\n",
        "    \"\"\" Test function for model and loader\n",
        "        when the model is ema model, will test the ema model\n",
        "        when the model is model, will test the regular model\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    if not args.no_progress:\n",
        "        test_loader = tqdm(test_loader, disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            data_time.update(time.time() - end)\n",
        "            model.eval()\n",
        "\n",
        "            inputs = inputs.to(args.device)\n",
        "            targets = targets.to(args.device)\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            losses.update(loss.item(), inputs.shape[0])\n",
        "            top1.update(prec1.item(), inputs.shape[0])\n",
        "            top5.update(prec5.item(), inputs.shape[0])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if not args.no_progress:\n",
        "                test_loader.set_description(\n",
        "                    \"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. \\\n",
        "                    Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \"\n",
        "                    .format(\n",
        "                        batch=batch_idx + 1,\n",
        "                        iter=len(test_loader),\n",
        "                        data=data_time.avg,\n",
        "                        bt=batch_time.avg,\n",
        "                        loss=losses.avg,\n",
        "                        top1=top1.avg,\n",
        "                        top5=top5.avg,\n",
        "                    ))\n",
        "        if not args.no_progress:\n",
        "            test_loader.close()\n",
        "\n",
        "    logger.info(\"Epoch {} top-1 acc: {:.2f}\".format(epoch, top1.avg))\n",
        "    logger.info(\"Epoch {} top-5 acc: {:.2f}\".format(epoch, top5.avg))\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFiDOtcCJHpW",
        "outputId": "18d9d160-1731-4c0d-882e-486afd376d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./Classification-SemiCLS/train_semi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the training"
      ],
      "metadata": {
        "id": "pS7Arvmk67lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running UCM with Fixmatch baseline\n",
        "!python3 ./Classification-SemiCLS/train_semi.py --cfg ./Classification-SemiCLS/configs/fm_ucm.py --gpu-id 0 --out ./Classification-SemiCLS/results/fixmatch/fm_ucm --seed 5 "
      ],
      "metadata": {
        "id": "1N8W_QFzSxU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd893bc-a575-4a22-d4ef-3753ded3246d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
            "  warnings.warn(\n",
            "2023-03-05 20:14:16.244943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-05 20:14:18.933016: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-05 20:14:18.951037: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-05 20:14:18.951076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-05 20:14:23,419 - WARNING - root -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "2023-03-05 20:14:23,420 - INFO - root -   {'amp': False, 'cfg': './Classification-SemiCLS/configs/fm_ucm.py', 'device': device(type='cuda', index=0), 'eval_steps': 1024, 'fp16': False, 'gpu_id': 0, 'local_rank': -1, 'n_gpu': 1, 'no_progress': False, 'other_args': '', 'out': './Classification-SemiCLS/results/fixmatch/fm_ucm', 'pretrained': None, 'resume': '', 'seed': 5, 'total_steps': 20480, 'use_BN': False, 'world_size': 1, 'writer': <torch.utils.tensorboard.writer.SummaryWriter object at 0x7f7fbfc70ee0>}\n",
            "2023-03-05 20:14:27,844 - INFO - root -   ***** Running training *****\n",
            "2023-03-05 20:14:27,845 - INFO - root -     Task = MyDataset\n",
            "2023-03-05 20:14:27,845 - INFO - root -     Num Label = 84\n",
            "2023-03-05 20:14:27,845 - INFO - root -     Num Epochs = 20\n",
            "2023-03-05 20:14:27,845 - INFO - root -     Batch size per GPU = 8\n",
            "2023-03-05 20:14:27,846 - INFO - root -     Total train batch size = 8\n",
            "2023-03-05 20:14:27,846 - INFO - root -     Total optimization steps = 20480\n",
            "2023-03-05 20:14:27,846 - INFO - root -   Config (path: ./Classification-SemiCLS/configs/fm_ucm.py): {'train': {'eval_step': 1024, 'total_steps': 20480, 'trainer': {'type': 'FixMatch', 'threshold': 0.95, 'T': 1.0, 'lambda_u': 1.0, 'loss_x': {'type': 'cross_entropy', 'reduction': 'mean'}, 'loss_u': {'type': 'cross_entropy', 'reduction': 'none'}, 'amp': False}}, 'num_classes': 21, 'model': {'type': 'wideresnet', 'depth': 28, 'widen_factor': 2, 'dropout': 0, 'num_classes': 21}, 'ucm_mean': (0.485, 0.456, 0.406), 'ucm_std': (0.229, 0.224, 0.225), 'data': {'type': 'MyDataset', 'num_workers': 0, 'num_labeled': 84, 'num_classes': 21, 'batch_size': 8, 'expand_labels': False, 'mu': 7, 'root': './UCMerced_LandUse/Images', 'labeled_names_file': './UCMerced_LandUse/Images/UCM_train.txt', 'test_names_file': './UCMerced_LandUse/Images/UCM_test.txt', 'lpipelines': [[{'type': 'RandomHorizontalFlip'}, {'type': 'RandomResizedCrop', 'size': 224, 'scale': (0.2, 1.0)}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}]], 'upipelinse': [[{'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': 256}, {'type': 'CenterCrop', 'size': 224}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}], [{'type': 'RandomHorizontalFlip'}, {'type': 'RandomResizedCrop', 'size': 224, 'scale': (0.2, 1.0)}, {'type': 'RandAugmentMC', 'n': 2, 'm': 10}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}]], 'vpipeline': [{'type': 'Resize', 'size': 256}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}], 'eval_step': 1024}, 'scheduler': {'type': 'cosine_schedule_with_warmup', 'num_warmup_steps': 0, 'num_training_steps': 20480}, 'ema': {'use': True, 'pseudo_with_ema': False, 'decay': 0.999}, 'amp': {'use': False, 'opt_level': 'O1'}, 'log': {'interval': 1}, 'ckpt': {'interval': 1000}, 'optimizer': {'type': 'SGD', 'lr': 0.03, 'momentum': 0.9, 'weight_decay': 0.001, 'nesterov': True}, 'resume': '', 'seed': 5}\n",
            "Traceback (most recent call last):\n",
            "  File \"./Classification-SemiCLS/train_semi.py\", line 617, in <module>\n",
            "    main()\n",
            "  File \"./Classification-SemiCLS/train_semi.py\", line 267, in main\n",
            "    train(args, cfg, labeled_trainloader, unlabeled_trainloader, test_loader,\n",
            "  File \"./Classification-SemiCLS/train_semi.py\", line 464, in train\n",
            "    loss_dict = model_trainer.compute_loss(\n",
            "  File \"/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/trainer/fixmatch.py\", line 100, in compute_loss\n",
            "    logits = model(inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/models/wideresnet.py\", line 167, in forward\n",
            "    feat = self.block2(feat)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/models/wideresnet.py\", line 75, in forward\n",
            "    return self.layer(x)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\", line 204, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/GeoPythonWorkshop_SSL/Classification-SemiCLS/models/wideresnet.py\", line 54, in forward\n",
            "    out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 2450, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 368.00 MiB (GPU 0; 14.75 GiB total capacity; 13.38 GiB already allocated; 88.81 MiB free; 13.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    }
  ]
}